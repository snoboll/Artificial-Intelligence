{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Numpy\n",
    "We compute the regression line linking counts of letters in a text to counts of letter a. \n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "We import the modules we need to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute the sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(X, y, w):\n",
    "    error = y - X @ w\n",
    "    return error.T @ error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Xy):\n",
    "    maxima = np.amax(Xy, axis=0)\n",
    "    D = np.diag(maxima)\n",
    "    D_inv = np.linalg.inv(D)\n",
    "    Xy = Xy @ D_inv\n",
    "    return (Xy, maxima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic descent\n",
    "Function to apply a stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoch_descent(X, y, alpha, w):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param alpha:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global logs, logs_stoch\n",
    "    logs = []\n",
    "    logs_stoch = []\n",
    "    random.seed(0)\n",
    "    idx = list(range(len(X)))\n",
    "    for epoch in range(500):\n",
    "        random.shuffle(idx)\n",
    "        w_old = w\n",
    "        for i in idx:\n",
    "            loss = y[i] - X[i] @ w\n",
    "            gradient = loss * X[i].reshape(-1, 1)\n",
    "            w = w + alpha * gradient\n",
    "            logs_stoch += (w, alpha, sse(X, y, w))\n",
    "        if np.linalg.norm(w - w_old) / np.linalg.norm(w) < 0.005:\n",
    "            print(\"Epoch\", epoch)\n",
    "            break\n",
    "        logs += (w, alpha, sse(X, y, w))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch descent\n",
    "Function to apply a batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_descent(X, y, alpha, w):\n",
    "    \"\"\"\n",
    "    Batch gradient descent\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param alpha:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global logs\n",
    "    logs = []\n",
    "    alpha /= len(X)\n",
    "    for epoch in range(1, 500):\n",
    "        loss = y - X @ w\n",
    "        gradient = X.T @ loss\n",
    "        w_old = w\n",
    "        w = w + alpha * gradient\n",
    "        logs += (w, alpha, sse(X, y, w))\n",
    "        if np.linalg.norm(w - w_old) / np.linalg.norm(w) < 0.0005:\n",
    "            print(\"Epoch\", epoch)\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load the data and we possibly normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = True\n",
    "debug = False\n",
    "X, y = datasets.load_tsv(\n",
    "    'https://raw.githubusercontent.com/pnugues/ilppp/master/programs/ch04/salammbo/salammbo_a_en.tsv')\n",
    "# Predictors\n",
    "X = np.array(X)\n",
    "# Response\n",
    "y = np.array([y]).T\n",
    "\n",
    "alpha = 1.0e-10\n",
    "if normalized:\n",
    "    X, maxima_X = normalize(X)\n",
    "    y, maxima_y = normalize(y)\n",
    "    maxima = np.concatenate((maxima_X, maxima_y))\n",
    "    alpha = 1.0\n",
    "    print(\"-Normalized-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply a batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Batch descent===\")\n",
    "w = np.zeros(X.shape[1]).reshape((-1, 1))\n",
    "w = batch_descent(X, y, alpha, w)\n",
    "print(\"Weights\", w)\n",
    "print(\"SSE\", sse(X, y, w))\n",
    "if normalized:\n",
    "    maxima = maxima.reshape(-1, 1)\n",
    "    w = maxima[-1, 0] * (w / maxima[:-1, 0:1])\n",
    "    print(\"Restored weights\", w)\n",
    "if debug:\n",
    "    print(\"Logs\", logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We restore the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fig = [X[i][1] * maxima_X[1] for i in range(len(X))]\n",
    "y_fig = [yi * maxima_y for yi in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the coordinates and the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fig, y_fig)\n",
    "plt.plot([min(x_fig), max(x_fig)],\n",
    "         [[1, min(x_fig)] @ w, [1, max(x_fig)] @ w])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs[2::3])), logs[2::3], c='b', marker='x')\n",
    "plt.title(\"Batch gradient descent: Sum of squared errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs[0::3])), list(map(lambda pair: pair[1], logs[0::3])), marker='o')\n",
    "for i in range(len(logs[0::3])):\n",
    "    plt.annotate(i, xy=logs[0::3][i])\n",
    "plt.title(\"Batch gradient descent: Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply a stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Stochastic descent===\")\n",
    "w = np.zeros(X.shape[1]).reshape((-1, 1))\n",
    "w = stoch_descent(X, y, alpha, w)\n",
    "print(\"Weights\", w)\n",
    "print(\"SSE\", sse(X, y, w))\n",
    "if normalized:\n",
    "    maxima = maxima.reshape(-1, 1)\n",
    "    w = maxima[-1, 0] * (w / maxima[:-1, 0:1])\n",
    "    print(\"Restored weights\", w)\n",
    "if debug:\n",
    "    print(\"Logs\", logs)\n",
    "    print(\"Logs stoch.\", logs_stoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fig, y_fig)\n",
    "plt.plot([min(x_fig), max(x_fig)],\n",
    "         [[1, min(x_fig)] @ w, [1, max(x_fig)] @ w])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The errors by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs[2::3])), logs[2::3], c='b', marker='x')\n",
    "plt.title(\"Stochastic gradient descent: Sum of squared errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs[0::3])), list(map(lambda pair: pair[1], logs[0::3])), marker='o')\n",
    "plt.title(\"Stochastic gradient descent: Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs_stoch[2::3])), logs_stoch[2::3], c='b', marker='x')\n",
    "plt.title(\"Stochastic gradient descent: Sum of squared errors (individual updates)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates by individual update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs_stoch[0::3])), list(map(lambda pair: pair[1], logs_stoch[0::3])),\n",
    "         marker='o')\n",
    "plt.title(\"Stochastic gradient descent: Weights (individual updates)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "We compute the regression line linking counts of letters in a text to counts of letter a using Python functions (no numpy).\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "We import the modules we need to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import vector\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute the sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(X, y, w):\n",
    "    error = vector.sub(y, vector.mul_mat_vec(X, w))\n",
    "    return vector.dot(error, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(observations):\n",
    "    maxima = [max([obs[i] for obs in observations]) for i in range(len(observations[0]))]\n",
    "    return ([[obs[i] / maxima[i]\n",
    "              for i in range(len(observations[0]))] for obs in observations],\n",
    "            maxima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic descent\n",
    "Function to apply a stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoch_descent(X, y, alpha, w):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param alpha:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global logs, logs_stoch\n",
    "    logs = []\n",
    "    logs_stoch = []\n",
    "    random.seed(0)\n",
    "    idx = list(range(len(X)))\n",
    "    for epoch in range(1000):\n",
    "        random.shuffle(idx)\n",
    "        w_old = w\n",
    "        for i in idx:\n",
    "            loss = y[i] - vector.dot(X[i], w)\n",
    "            gradient = vector.mul(loss, X[i])\n",
    "            w = vector.add(w, vector.mul(alpha, gradient))\n",
    "            logs_stoch += (w, alpha, sse(X, y, w))\n",
    "        if vector.norm(vector.sub(w, w_old)) / vector.norm(w) < 1.0e-5:\n",
    "            break\n",
    "        logs += (w, alpha, sse(X, y, w))\n",
    "    print(\"Epoch\", epoch)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch descent\n",
    "Function to apply a batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_descent(X, y, alpha, w):\n",
    "    \"\"\"\n",
    "    Batch gradient descent\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param alpha:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global logs\n",
    "    logs = []\n",
    "    alpha /= len(X)\n",
    "    for epoch in range(1, 1000):\n",
    "        loss = vector.sub(y, vector.mul_mat_vec(X, w))\n",
    "        gradient = vector.mul_mat_vec(vector.transpose(X), loss)\n",
    "        w_old = w\n",
    "        w = vector.add(w, vector.mul(alpha, gradient))\n",
    "        logs += (w, alpha, sse(X, y, w))\n",
    "        if vector.norm(vector.sub(w, w_old)) / vector.norm(w) < 1.0e-5:\n",
    "            break\n",
    "    print(\"Epoch\", epoch)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load the data and we possibly normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = True\n",
    "debug = False\n",
    "X, y = datasets.load_tsv(\n",
    "    'https://raw.githubusercontent.com/pnugues/ilppp/master/programs/ch04/salammbo/salammbo_a_en.tsv')\n",
    "\n",
    "alpha = 1.0e-10\n",
    "if normalized:\n",
    "    X, maxima_X = normalize(X)\n",
    "    maxima_y = max(y)\n",
    "    y = [yi / maxima_y for yi in y]\n",
    "    maxima = maxima_X + [maxima_y]\n",
    "    alpha = 1.0\n",
    "    print(\"-Normalized-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply a batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Batch descent===\")\n",
    "w = [0.0] * (len(X))\n",
    "w = batch_descent(X, y, alpha, w)\n",
    "print(\"Weights\", w)\n",
    "print(\"SSE\", sse(X, y, w))\n",
    "if normalized:\n",
    "    w = [w[i] * maxima[-1] / maxima[i] for i in range(len(w))]\n",
    "    print(\"Restored weights\", w)\n",
    "if debug:\n",
    "    print(\"Logs\", logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We restore the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fig = [X[i][1] * maxima_X[1] for i in range(len(X))]\n",
    "y_fig = [yi * maxima_y for yi in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the coordinates and the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fig, y_fig)\n",
    "plt.plot([min(x_fig), max(x_fig)],\n",
    "         [vector.dot([1, min(x_fig)], w), \n",
    "          vector.dot([1, max(x_fig)], w)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs[2::3])), logs[2::3], c='b', marker='x')\n",
    "plt.title(\"Batch gradient descent: Sum of squared errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs[0::3])), list(map(lambda pair: pair[1], logs[0::3])), marker='o')\n",
    "for i in range(len(logs[0::3])):\n",
    "    plt.annotate(i, xy=logs[0::3][i])\n",
    "plt.title(\"Batch gradient descent: Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply a stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Stochastic descent===\")\n",
    "w = [0.0] * (len(X))\n",
    "w = stoch_descent(X, y, alpha, w)\n",
    "print(\"Weights\", w)\n",
    "print(\"SSE\", sse(X, y, w))\n",
    "if normalized:\n",
    "    w = [w[i] * maxima[-1] / maxima[i] for i in range(len(w))]\n",
    "    print(\"Restored weights\", w)\n",
    "if debug:\n",
    "    print(\"Logs\", logs)\n",
    "    print(\"Logs stoch.\", logs_stoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fig, y_fig)\n",
    "plt.plot([min(x_fig), max(x_fig)],\n",
    "         [vector.dot([1, min(x_fig)], w), \n",
    "          vector.dot([1, max(x_fig)], w)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The errors by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs[2::3])), logs[2::3], c='b', marker='x')\n",
    "plt.title(\"Stochastic gradient descent: Sum of squared errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs[0::3])), list(map(lambda pair: pair[1], logs[0::3])), marker='o')\n",
    "plt.title(\"Stochastic gradient descent: Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(logs_stoch[2::3])), logs_stoch[2::3], c='b', marker='x')\n",
    "plt.title(\"Stochastic gradient descent: Sum of squared errors (individual updates)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight updates by individual update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda pair: pair[0], logs_stoch[0::3])), list(map(lambda pair: pair[1], logs_stoch[0::3])),\n",
    "         marker='o')\n",
    "plt.title(\"Stochastic gradient descent: Weights (individual updates)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
